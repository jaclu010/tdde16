{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Students:__ Sebastian Callh sebca553, Jacob Lundberg jaclu010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus for this assignment will be at least 1000 Googla Play app descriptions. To acquire those we crawl all the categories for their presented app urls, and then those app urls for their description. First off let's import used packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import urllib.request\n",
    "import nltk\n",
    "import pickle\n",
    "from functools import reduce\n",
    "from itertools import chain\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape the urls we use `urllib` and `re` with the helper functions defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "catreg = r'<a class=\\\"child-submenu-link\\\" href=\\\"(/store/apps/category/.*?)\\\" title=\\\".*?\\\" jsl=\\\"\\$x 5;\\\" jsan=\\\"7.child-submenu-link,8.href,0.title\\\">.*?<\\/a>'\n",
    "catre = re.compile(catreg)\n",
    "appreg = r'href=\\\"(/store/apps/details.*?)\\\"'\n",
    "appre = re.compile(appreg)\n",
    "\n",
    "play_url = 'https://play.google.com'\n",
    "def scrape_cat_urls(url):\n",
    "    mkdwn = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    return re.findall(catre, mkdwn)\n",
    "\n",
    "def scrape_app_urls(cat_url):\n",
    "    mkdwn = urllib.request.urlopen(play_url + cat_url).read().decode('utf-8')\n",
    "    return re.findall(appre, mkdwn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fill the quota of 1000 descriptions, some additional links are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1179"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_urls = scrape_cat_urls(play_url + '/store/apps')\n",
    "cat_urls.append('/store/search?q=poop&c=apps')\n",
    "cat_urls.append('/store/apps/new')\n",
    "cat_urls.append('/store/apps/top')\n",
    "app_urls = set(reduce(lambda lst, url: lst + scrape_app_urls(url), cat_urls, []))\n",
    "len(app_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When scraping all the chosen pages we are well above the requirement, so we can now scrape the actual app descriptions (together with the app names). The descriptions are stored in a pickle file to avoid re-scraping the web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = r'itemprop=\\\"description.*?\\\">.*?<div jsname=\\\".*?\\\">(.*?)</div>'\n",
    "name = r'itemprop=\\\"name\\\" content=\\\"(.*?)\\\"\\/>'\n",
    "app_desc_re = re.compile(desc)\n",
    "app_name_re = re.compile(name)\n",
    "\n",
    "def scrape_app(app_url):\n",
    "    mkdwn = urllib.request.urlopen(play_url + app_url + '&hl=en').read().decode('utf-8')\n",
    "    desc = re.findall(app_desc_re, mkdwn)[0]\n",
    "    name = re.findall(app_name_re, mkdwn)[0]\n",
    "    return name, desc\n",
    "\n",
    "apps = [scrape_app(url) for url in app_urls]\n",
    "with open('app-descriptions.pkl', 'wb') as f:\n",
    "    pickle.dump(apps, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and tfidf representation\n",
    "With the corpus acquired, we can now creat ifidf representations for the descriptions. To do this we use the `TfidfVectorizer` from `sklearn` which constructs the vectors as needed, and also allows us to specify our own tokenizer function and stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stopwords():\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    path = './nltk_data/corpora/stopwords/english'\n",
    "    with open(path) as f:\n",
    "        stopwords = f.readlines()\n",
    "        stopwords = [x.strip() for x in stopwords]\n",
    "        stopwords.append('br')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True) # we need to think about where stopwords need to show up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import Counter\n",
    "#from sklearn.preprocessing import normalize\n",
    "\n",
    "def tokenize(d): \n",
    "    tokens = [s.lower() for s in nltk.word_tokenize(d) if s.isalpha()]\n",
    "    return [stemmer.stem(w) for w in tokens if not w in stopwords] # we need to think about where stopwords need to show up\n",
    "\n",
    "#docs = [re.findall(appre, d)[0] for d in descriptions]\n",
    "#names = [re.findall(appname, d)[0] for d in descriptions]\n",
    "#tokens = reduce(lambda lst, d: lst + [process(d)], docs, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Inverted file index (Vector Model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Preprocess text using NLP techniques from __[nltk module](http://www.nltk.org/py-modindex.html)__ or spaCy.\n",
    "\n",
    "Using nltk.download(ID) to get the corpora if it is not downloaded before. __[nltk corpora](http://www.nltk.org/nltk_data/)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...)Compute tdidf \n",
    "eg. Using functions from __[scikit-learn module](http://scikit-learn.org/stable/modules/classes.html)__. TfidfVectorizer is used for converting a collection of raw documents to a matrix of TF-IDF features.\n",
    "#### You can also build the tfidf matrix with other library or your own algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "transvector = TfidfVectorizer(tokenizer=tokenize, stop_words=stopwords, analyzer = \"word\") # we need to think about where stopwords need to show up\n",
    "app_descrs = [app[1] for app in apps]\n",
    "vocab_matrix = transvector.fit_transform(app_descrs)\n",
    "\n",
    "\n",
    "\n",
    "# Non-english words are not stemmed correctly (!)!(!()\")\n",
    "\n",
    "# --(!)-- WARNING --(!)-- \n",
    "# #造造 toxic material above 造造#  \n",
    "\n",
    "#        -_('_')_-\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#[\n",
    "#     'This is the first document.',\n",
    "#     'This is the second second document.',\n",
    "#     'And this is the third one.',\n",
    "#     'Is this the first document?',]\n",
    "#print(X.toarray())\n",
    "#print(transvector.get_feature_names())\n",
    "\n",
    "#def tfidf(corpus):\n",
    "#    return transvector.fit_transform(corpus)\n",
    "\n",
    "# idfs.item.toarray() to call all \n",
    "#idfs = [tfidf(corp) for corp in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Process\n",
    "\n",
    "eg. \"Dragon, Control, hero, running\"\n",
    "\n",
    "eg. \"The hero controls the dragon to run.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_max(arr, n):\n",
    "    indices = arr.ravel().argsort()[-n:]\n",
    "    indices = (np.unravel_index(i, arr.shape) for i in indices)\n",
    "    return reversed([(arr[i], i[1]) for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(qstring, k):\n",
    "    q = transvector.transform([qstring])\n",
    "    \n",
    "    res = cosine_similarity(q, vocab_matrix)\n",
    "    \n",
    "    top_k = n_max(res, k)\n",
    "\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "qres = query(\"ponyville\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 1165  -  My Little Pony Rainbow Runners  -  0.12055867793548761 . Desc: Run, jump, fly and restore the colors of the world ...\n",
      "ID: 1178  -  Deep Sleep and Relax Hypnosis  -  0.0 . Desc: Do you have trouble sleeping or getting into a rel ...\n",
      "ID: 394  -  Kids Tap and Color (Lite)  -  0.0 . Desc: Coloring Book Tap &amp; Color is an interactive co ...\n",
      "ID: 388  -  NFL  -  0.0 . Desc: The official app of the NFL is the best, pure foot ...\n",
      "ID: 389  -  Chitose.  A crypto currency prices viewer on wear  -  0.0 . Desc: Chitose is a crypto currency prices viewer on your ...\n"
     ]
    }
   ],
   "source": [
    "for row in qres:\n",
    "    print(\"ID:\", row[1], \" - \", apps[row[1]][0], \" - \", row[0], \". Desc:\", apps[row[1]][1][0:50], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Soy Luna - Hits Music Lyrics',\n",
       " 'Sing by reading the best music lyrics Soy Luna - Hits Music Lyrics.<br>You can hear Soy Luna music - Hits Music Lyrics anytime and anywhere you need.<br><br>Simply download you can get all of Soy Luna&#39;s music on your mobile.<br><br>soy luna open music en la mansion<br>soy luna open music ambar y matteo<br>soy luna open music a rodar mi vida<br>soy luna open amber music delfi y jazmin<br>soy luna open music ambre<br>soy luna open music auslosung<br>soy luna open music alzo mi bandera<br>soy luna open music amber canta mirame a mi<br>soy luna open music auf deutsch<br>soy luna open music i&#39;d be crazy<br>soy luna open music roller band<br>soy luna open music cuando bailo<br>Bailes de soy luna open music<br>soy luna open music ad be crazy<br>soy luna open music con letra<br>soy luna open music chica vs chico completo letra<br>soy luna open music chica vs chica<br>soy luna open music chica vs chico completo capitulo<br>soy luna open music corazo<br>soy luna open music como me ves<br>soy luna open music chica vs chico letra<br>soy luna open music<br>soy luna open music despedida<br><br><br>Disclaimer:<br>The app is not official, it&#39;s just the music player not the official of the artist.<br>So enjoy with this app, and do not forget to give us five ratting. Thank you!')"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apps[899]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py venv",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
