{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Students:__ Sebastian Callh sebca553, Jacob Lundberg jaclu010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling\n",
    "The corpus for this assignment will be at least 1000 Googla Play app descriptions. To acquire those we crawl all the categories for their presented app urls, and then those app urls for their description. First off let's import used packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import urllib.request\n",
    "import nltk\n",
    "import pickle\n",
    "from functools import reduce\n",
    "from itertools import chain\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape the urls we use `urllib` and `re` with the helper functions defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "catreg = r'<a class=\\\"child-submenu-link\\\" href=\\\"(/store/apps/category/.*?)\\\" title=\\\".*?\\\" jsl=\\\"\\$x 5;\\\" jsan=\\\"7.child-submenu-link,8.href,0.title\\\">.*?<\\/a>'\n",
    "catre = re.compile(catreg)\n",
    "appreg = r'href=\\\"(/store/apps/details.*?)\\\"'\n",
    "appre = re.compile(appreg)\n",
    "\n",
    "play_url = 'https://play.google.com'\n",
    "def scrape_cat_urls(url):\n",
    "    mkdwn = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    return re.findall(catre, mkdwn)\n",
    "\n",
    "def scrape_app_urls(cat_url):\n",
    "    mkdwn = urllib.request.urlopen(play_url + cat_url).read().decode('utf-8')\n",
    "    return re.findall(appre, mkdwn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fill the quota of 1000 descriptions, some additional links are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1259"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_urls = scrape_cat_urls(play_url + '/store/apps')\n",
    "cat_urls.append('/store/search?q=poop&c=apps')\n",
    "cat_urls.append('/store/apps/new')\n",
    "cat_urls.append('/store/apps/top')\n",
    "app_urls = set(reduce(lambda lst, url: lst + scrape_app_urls(url), cat_urls, []))\n",
    "len(app_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When scraping all the chosen pages we are well above the requirement, so we can now scrape the actual app descriptions (together with the app names). The descriptions are stored in a pickle file to avoid re-scraping the web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-70eaf8f4bf9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'app-descriptions.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mapps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "#with open('app-descriptions.pkl', 'rb') as f:\n",
    "#    apps = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = r'itemprop=\\\"description.*?\\\">.*?<div jsname=\\\".*?\\\">(.*?)</div>'\n",
    "name = r'itemprop=\\\"name\\\" content=\\\"(.*?)\\\"\\/>'\n",
    "app_desc_re = re.compile(desc)\n",
    "app_name_re = re.compile(name)\n",
    "\n",
    "def scrape_app(app_url):\n",
    "    mkdwn = urllib.request.urlopen(play_url + app_url + '&hl=en').read().decode('utf-8')\n",
    "    desc = re.findall(app_desc_re, mkdwn)[0]\n",
    "    name = re.findall(app_name_re, mkdwn)[0]\n",
    "    return name, desc\n",
    "\n",
    "apps = [scrape_app(url) for url in app_urls]\n",
    "with open('app-descriptions.pkl', 'wb') as f:\n",
    "    pickle.dump(apps, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and tfidf representation\n",
    "With the corpus acquired, we can now creat ifidf representations for the descriptions. To do this we use the `TfidfVectorizer` from `sklearn` which constructs the vectors as needed, and also allows us to specify our own tokenizer function and stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to ./nltk...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "def load_stopwords():\n",
    "\n",
    "    nltk_path = './nltk'\n",
    "    if not os.path.exists(nltk_path):\n",
    "        os.makedirs(nltk_path)\n",
    "     \n",
    "    nltk.download('stopwords', download_dir=nltk_path)\n",
    "\n",
    "    custom_words =  ['br']\n",
    "    with open(nltk_path + '/corpora/stopwords/english') as f:\n",
    "        nltk_words = [x.strip() for x in f.readlines()]\n",
    "        \n",
    "    return custom_words + nltk_words\n",
    "    \n",
    "stopwords = load_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(d): \n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    tokens = [s.lower() for s in nltk.word_tokenize(d) if s.isalpha()]\n",
    "    return [stemmer.stem(w) for w in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Inverted file index (Vector Model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Preprocess text using NLP techniques from __[nltk module](http://www.nltk.org/py-modindex.html)__ or spaCy.\n",
    "\n",
    "Using nltk.download(ID) to get the corpora if it is not downloaded before. __[nltk corpora](http://www.nltk.org/nltk_data/)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...)Compute tdidf \n",
    "eg. Using functions from __[scikit-learn module](http://scikit-learn.org/stable/modules/classes.html)__. TfidfVectorizer is used for converting a collection of raw documents to a matrix of TF-IDF features.\n",
    "#### You can also build the tfidf matrix with other library or your own algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "transvector = TfidfVectorizer(tokenizer=tokenize, stop_words=stopwords, analyzer = \"word\") # we need to think about where stopwords need to show up\n",
    "app_descrs = [app[1] for app in apps]\n",
    "vocab_matrix = transvector.fit_transform(app_descrs)\n",
    "\n",
    "\n",
    "\n",
    "# Non-english words are not stemmed correctly (!)!(!()\")\n",
    "\n",
    "# --(!)-- WARNING --(!)-- \n",
    "# #造造 toxic material above 造造#  \n",
    "\n",
    "#        -_('_')_-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Process\n",
    "\n",
    "eg. \"Dragon, Control, hero, running\"\n",
    "\n",
    "eg. \"The hero controls the dragon to run.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_max(arr, n):\n",
    "    indices = arr.ravel().argsort()[-n:]\n",
    "    indices = (np.unravel_index(i, arr.shape) for i in indices)\n",
    "    return reversed([(arr[i], i[1]) for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(qstring, k):\n",
    "    q = transvector.transform([qstring])\n",
    "    \n",
    "    res = cosine_similarity(q, vocab_matrix)\n",
    "    \n",
    "    top_k = n_max(res, k)\n",
    "\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "qres = query(\"The hero controls the dragon to run.\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 76  -  School of Dragons  -  0.4469180079438126 . Desc: Join Hiccup, Toothless, Astrid and the rest of the ...\n",
      "ID: 1246  -  Merge Dragons!  -  0.3653360600610415 . Desc: Discover dragon legends, magic, quests, and a secr ...\n",
      "ID: 872  -  Dragon Mania Legends  -  0.34055021729744117 . Desc: &quot;Dragon Mania Legends is for anyone that want ...\n",
      "ID: 566  -  Heroes of Warland - PvP Shooter Arena  -  0.3180317269817844 . Desc: Heroes of Warland is the most competitive online P ...\n",
      "ID: 988  -  Battle Arena: Heroes Adventure - Online RPG  -  0.3020582940150625 . Desc: Battle Arena: Heroes Adventure is an incredible mi ...\n"
     ]
    }
   ],
   "source": [
    "for row in qres:\n",
    "    print(\"ID:\", row[1], \" - \", apps[row[1]][0], \" - \", row[0], \". Desc:\", apps[row[1]][1][0:50], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py venv",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
